Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/uufs/chpc.utah.edu/common/home/u1000771/miniconda3/envs/cs6966/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.weight', 'pooler.dense.weight', 'pooler.dense.bias', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Traceback (most recent call last):
  File "assignment3.py", line 127, in <module>
    main(args)
  File "assignment3.py", line 116, in main
    exp_model.explain(obj["review"], os.path.join(args.output_dir,f'example_{idx}'))
  File "assignment3.py", line 82, in explain
    attributes, delta = lig.attribute(inputs=inputs,
  File "/uufs/chpc.utah.edu/common/home/u1000771/miniconda3/envs/cs6966/lib/python3.8/site-packages/captum/log/__init__.py", line 42, in wrapper
    return func(*args, **kwargs)
  File "/uufs/chpc.utah.edu/common/home/u1000771/miniconda3/envs/cs6966/lib/python3.8/site-packages/captum/attr/_core/layer/layer_integrated_gradients.py", line 494, in attribute
    attributions = self.ig.attribute.__wrapped__(  # type: ignore
  File "/uufs/chpc.utah.edu/common/home/u1000771/miniconda3/envs/cs6966/lib/python3.8/site-packages/captum/attr/_core/integrated_gradients.py", line 286, in attribute
    attributions = self._attribute(
  File "/uufs/chpc.utah.edu/common/home/u1000771/miniconda3/envs/cs6966/lib/python3.8/site-packages/captum/attr/_core/integrated_gradients.py", line 351, in _attribute
    grads = self.gradient_func(
  File "/uufs/chpc.utah.edu/common/home/u1000771/miniconda3/envs/cs6966/lib/python3.8/site-packages/captum/attr/_core/layer/layer_integrated_gradients.py", line 470, in gradient_func
    output = _run_forward(
  File "/uufs/chpc.utah.edu/common/home/u1000771/miniconda3/envs/cs6966/lib/python3.8/site-packages/captum/_utils/common.py", line 482, in _run_forward
    output = forward_func(
  File "assignment3.py", line 36, in forward_func
    pred = self.__pipeline.model(inputs,
  File "/uufs/chpc.utah.edu/common/home/u1000771/miniconda3/envs/cs6966/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/uufs/chpc.utah.edu/common/home/u1000771/miniconda3/envs/cs6966/lib/python3.8/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py", line 1314, in forward
    outputs = self.deberta(
  File "/uufs/chpc.utah.edu/common/home/u1000771/miniconda3/envs/cs6966/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/uufs/chpc.utah.edu/common/home/u1000771/miniconda3/envs/cs6966/lib/python3.8/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py", line 1084, in forward
    encoder_outputs = self.encoder(
  File "/uufs/chpc.utah.edu/common/home/u1000771/miniconda3/envs/cs6966/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/uufs/chpc.utah.edu/common/home/u1000771/miniconda3/envs/cs6966/lib/python3.8/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py", line 520, in forward
    output_states = layer_module(
  File "/uufs/chpc.utah.edu/common/home/u1000771/miniconda3/envs/cs6966/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/uufs/chpc.utah.edu/common/home/u1000771/miniconda3/envs/cs6966/lib/python3.8/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py", line 362, in forward
    attention_output = self.attention(
  File "/uufs/chpc.utah.edu/common/home/u1000771/miniconda3/envs/cs6966/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/uufs/chpc.utah.edu/common/home/u1000771/miniconda3/envs/cs6966/lib/python3.8/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py", line 293, in forward
    self_output = self.self(
  File "/uufs/chpc.utah.edu/common/home/u1000771/miniconda3/envs/cs6966/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/uufs/chpc.utah.edu/common/home/u1000771/miniconda3/envs/cs6966/lib/python3.8/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py", line 727, in forward
    rel_att = self.disentangled_attention_bias(
  File "/uufs/chpc.utah.edu/common/home/u1000771/miniconda3/envs/cs6966/lib/python3.8/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py", line 834, in disentangled_attention_bias
    score += p2c_att / scale.to(dtype=p2c_att.dtype)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 886.00 MiB (GPU 0; 44.55 GiB total capacity; 41.93 GiB already allocated; 290.25 MiB free; 43.95 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
